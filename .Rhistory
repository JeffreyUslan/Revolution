for(i in 1:num.iter ) {
C =
values(
from.dfs(
mapreduce(
P,
map = kmeans.map,
reduce = kmeans.reduce)))
if(combine || in.memory.combine)
C = C[, -1]/C[, 1]
## @knitr end
#      points(C, col = i + 1, pch = 19)
## @knitr kmeans-main-2
if(nrow(C) < num.clusters) {
C =
rbind(
C,
matrix(
rnorm(
(num.clusters -
nrow(C)) * nrow(C)),
ncol = nrow(C)) %*% C) }}
C}
out = list()
for(be in c("local", "hadoop")) {
rmr.options(backend = be)
set.seed(0)
## @knitr kmeans-data
P =
do.call(
rbind,
rep(
list(
matrix(
rnorm(10, sd = 10),
ncol=2)),
20)) +
matrix(rnorm(200), ncol =2)
## @knitr end
#  x11()
#  plot(P)
#  points(P)
out[[be]] =
## @knitr kmeans-run
kmeans.mr(
to.dfs(P),
num.clusters = 12,
num.iter = 5,
combine = FALSE,
in.memory.combine = FALSE)
## @knitr end
}
romeo_and_juliet=readLines("./romeo_and_juliet.txt",skipNul = TRUE)
romeo_and_juliet=as.character(sapply(romeo_and_juliet,function(x) x=iconv(x, "latin1", "ASCII", sub="")))
romeo_and_juliet=as.character(sapply(romeo_and_juliet,function(x) x=gsub("[^[:alpha:] ]", "",x)[[1]]))
romeo_and_juliet=as.character(sapply(romeo_and_juliet,tolower))
romeo_and_juliet=to.dfs(romeo_and_juliet)
#function that encapsulates the job
#     Maps what a word is
wc.map =  function(., lines) {
words.list=strsplit(x = lines,split = " ")
words=unlist(words.list)
keyval(words, 1)}
#The reducing function to count the words
wc.reduce = function(words, counts ) {
keyval(words, sum(counts))}
# combine the map function and the reduce function
wordcount=function(input,output=NULL){
mapreduce(
input = input,
output = output,
input.format = "text",
map = wc.map,
reduce = wc.reduce,
combine = T)
}
outcome=from.dfs(wordcount(input=romeo_and_juliet))
library(rmr2)
rmr.options(backend="local")
romeo_and_juliet=readLines("./romeo_and_juliet.txt",skipNul = TRUE)
romeo_and_juliet=as.character(sapply(romeo_and_juliet,function(x) x=iconv(x, "latin1", "ASCII", sub="")))
romeo_and_juliet=as.character(sapply(romeo_and_juliet,function(x) x=gsub("[^[:alpha:] ]", "",x)[[1]]))
romeo_and_juliet=as.character(sapply(romeo_and_juliet,tolower))
romeo_and_juliet=to.dfs(romeo_and_juliet)
wc.map =  function(., lines) {
words.list=strsplit(x = lines,split = " ")
words=unlist(words.list)
keyval(words, 1)}
wc.reduce = function(words, counts ) {
keyval(words, sum(counts))}
wordcount=function(input,output=NULL){
mapreduce(
input = input,
output = output,
input.format = "text",
map = wc.map,
reduce = wc.reduce,
combine = T)
}
outcome=from.dfs(wordcount(input=romeo_and_juliet))
library(rhdfs)
install.packages("rhdfs")
install.packages("C:/Users/Jeffrey/Downloads/rhdfs_1.0.8.zip", repos = NULL,type="binary")
library(rhdfs)
Sys.setenv("HADOOP_CMD"="/usr/local/hadoop/bin/hadoop")
library(rhdfs)
Sys.setenv("HADOOP_STREAMING"="/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.4.0.jar")
library(rhdfs)
romeo_and_juliet=readLines("./romeo_and_juliet.txt",skipNul = TRUE)
romeo_and_juliet=as.character(sapply(romeo_and_juliet,function(x) x=iconv(x, "latin1", "ASCII", sub="")))
romeo_and_juliet=as.character(sapply(romeo_and_juliet,function(x) x=gsub("[^[:alpha:] ]", "",x)[[1]]))
romeo_and_juliet=as.character(sapply(romeo_and_juliet,tolower))
hdfs.init()
romeo_and_juliet=to.dfs(romeo_and_juliet)
wc.map =  function(k, lines) {
words.list=strsplit(x = lines,split = " ")
words=unlist(words.list)
keyval(words, 1)}
wc.reduce = function(words, counts ) {
keyval(words, sum(counts))}
wordcount=function(input,output=NULL){
mapreduce(
input = input,
output = output,
input.format = "text",
map = wc.map,
reduce = wc.reduce,
combine = T)
}
outcome=from.dfs(wordcount(input=romeo_and_juliet))
romeo_and_juliet=readLines("./romeo_and_juliet.txt",skipNul = TRUE)
romeo_and_juliet=as.character(sapply(romeo_and_juliet,function(x) x=iconv(x, "latin1", "ASCII", sub="")))
romeo_and_juliet=as.character(sapply(romeo_and_juliet,function(x) x=gsub("[^[:alpha:] ]", "",x)[[1]]))
romeo_and_juliet=as.character(sapply(romeo_and_juliet,tolower))
#function that encapsulates the job
#     Maps what a word is
wc.map =  function(k, lines) {
words.list=strsplit(x = lines,split = " ")
words=unlist(words.list)
keyval(words, 1)}
#The reducing function to count the words
wc.reduce = function(words, counts ) {
keyval(words, sum(counts))}
# combine the map function and the reduce function
wordcount=function(input,output=NULL){
mapreduce(
input = input,
output = output,
map = wc.map,
reduce = wc.reduce,
combine = TRUE)
#       input.format = "text",
}
rmr.options(backend="local")
outcome=from.dfs(wordcount(to.dfs(keyval(NULL,romeo_and_juliet))))
outcome
t=data.frame(key=outcome$key,val=as.numeric(outcome$val))
ord=order(t$val,decreasing=TRUE)
head(t[ord,])
t[ord,1:50]
dim(t)
t[ord,]
t[ord,][1:50,]
?strlen
head(sapply(t,nchar))
class(t)
dim(t)
lengths=sapply(t[,1],nchar)
class(t[,1])
class(t[,2])
class(t[,1])="character"
t[ord,][1:50,]
t=data.frame(key=outcome$key,val=as.numeric(outcome$val))
t[ord,][1:50,]
class(t[,1])
head(as.character(t[,1]))
t[,1]=as.character(t[,1])
t[ord,][1:50,]
lengths=sapply(t[,1],nchar)
head(lengths)
t[ord,][which(lengths>2),][1:50,]
t=data.frame(key=outcome$key,val=as.numeric(outcome$val))
t[,1]=as.character(t[,1])
lengths=sapply(t[,1],nchar)
t=t[which(lengths>2),]
ord=order(t$val,decreasing=TRUE)
t[ord,][1:50,]
t=data.frame(key=outcome$key,val=as.numeric(outcome$val))
t[,1]=as.character(t[,1])
lengths=sapply(t[,1],nchar)
t=t[which(lengths>3),]
ord=order(t$val,decreasing=TRUE)
t[ord,][1:50,]
t=data.frame(key=outcome$key,val=as.numeric(outcome$val))
t[,1]=as.character(t[,1])
lengths=sapply(t[,1],nchar)
t=t[which(lengths>4),]
ord=order(t$val,decreasing=TRUE)
t[ord,][1:50,]
row.names(t)=NULL
t[ord,][1:50,]
library(rmr2)
rmr.options(backend="local")
lr.map =function(., M) {
Y = M[,1]
X = M[,-1]
keyval(
1,
Y * X *
g(-Y * as.numeric(X %*% t(plane))))
}
lr.reduce = function(k, Z) {
keyval(k, t(as.matrix(apply(Z,2,sum))))
}
g = function(z) {
1/(1 + exp(-z))
for (i in 1:iterations) {
gradient =
values(
from.dfs(
mapreduce(
input,
map = lr.map,
reduce = lr.reduce,
combine = TRUE)))
plane = plane + alpha * gradient }
}
logistic.regression =
function(input, iterations, dims, alpha){
## Map function- makes a subset grid
lr.map =  function(., M) {
Y = M[,1]
X = M[,-1]
keyval(
1,
Y * X *
g(-Y * as.numeric(X %*% t(plane))))
}
## Reduce Function sum the dubset grids contribution
lr.reduce = function(k, Z) {
keyval(k, t(as.matrix(apply(Z,2,sum))))
}
## Sigmoid Function
g = function(z) {
1/(1 + exp(-z))
}
#Setting the plane
plane = t(rep(0, dims))
#Iterating
for (i in 1:iterations) {
gradient =
values(
from.dfs(
mapreduce(
input,
map = lr.map,
reduce = lr.reduce,
combine = TRUE)))
plane = plane + alpha * gradient }
plane }
test.size = 10^5
eps = rnorm(test.size)
testdata =
to.dfs(
as.matrix(
data.frame(
y = 2 * (eps > 0) - 1,
x1 = 1:test.size,
x2 = 1:test.size + eps)))
logistic.regression(testdata, 3, 2, 0.05)
library(rmr2)
rmr.options(backend="local")
kmeans.mr =
function(
P,
num.clusters,
num.iter,
combine,
in.memory.combine) {
#Calculate the distances
dist.fun =
function(C, P) {
apply(
C,
1,
function(x)
colSums((t(P) - x)^2))}
kmeans.map =
function(., P) {
nearest = {
if(is.null(C))
sample(
1:num.clusters,
nrow(P),
replace = TRUE)
else {
D = dist.fun(C, P)
nearest = max.col(-D)}}
if(!(combine || in.memory.combine))
keyval(nearest, P)
else
keyval(nearest, cbind(1, P))}
kmeans.reduce = {
if (!(combine || in.memory.combine) )
function(., P)
t(as.matrix(apply(P, 2, mean)))
else
function(k, P)
keyval(
k,
t(as.matrix(apply(P, 2, sum))))}
C = NULL
for(i in 1:num.iter ) {
C =
values(
from.dfs(
mapreduce(
P,
map = kmeans.map,
reduce = kmeans.reduce)))
if(combine || in.memory.combine)
C = C[, -1]/C[, 1]
if(nrow(C) < num.clusters) {
C =
rbind(
C,
matrix(
rnorm(
(num.clusters -
nrow(C)) * nrow(C)),
ncol = nrow(C)) %*% C) }}
C}
P =
do.call(
rbind,
rep(
list(
matrix(
rnorm(10, sd = 10),
ncol=2)),
20)) +
matrix(rnorm(200), ncol =2)
summary(P)
dim(P)
plot(P)
outcome=    kmeans.mr(
to.dfs(P),
num.clusters = 12,
num.iter = 5,
combine = FALSE,
in.memory.combine = FALSE)
}
summary(outcome)
summary(P)
View(outcome)
View(P)
outcome=    kmeans.mr(
to.dfs(P),
num.clusters = 3,
num.iter = 5,
combine = FALSE,
in.memory.combine = FALSE)
summary(P)
outcome
dist(P[1,],outcome[1])
dist(P[1,],outcome[1,])
P[1,],
P[1,]
outcome[1,]
x=P[1,]
sub_dist=apply(outcome,1,function(y){
sqrt((y[1]-x[1])^2+(y[2]-x[2])^2)
})
sub_dist
min(sub_dist)
which(sub_dist==min(sub_dist))
clust_ind=apply(P,1,function(x){
sub_dist=apply(outcome,1,function(y){
sqrt((y[1]-x[1])^2+(y[2]-x[2])^2)
})
which(sub_dist==min(sub_dist))
})
clust_ind
P[,3]=apply(P,1,function(x){
sub_dist=apply(outcome,1,function(y){
sqrt((y[1]-x[1])^2+(y[2]-x[2])^2)
})
which(sub_dist==min(sub_dist))
})
P[,3]=apply(P,1,function(x){
sub_dist=apply(outcome,1,function(y){
sqrt((y[1]-x[1])^2+(y[2]-x[2])^2)
})
which(sub_dist==min(sub_dist))
})
P[,3]=clust_ind
dim(P)
P=cbind(P,clust_ind)
View(P)
ggplot(P)+geom_point(aes(x=V1,y=V2))
library(ggplot2)
ggplot(P)+geom_point(aes(x=V1,y=V2))
P=as.data.frame(P)
ggplot(P)+geom_point(aes(x=V1,y=V2))
ggplot(P)+geom_point(aes(x=V1,y=V2),colour=clust_ind)
?kmeans
kmeans(as.matrix(P),3)
k=kmeans(as.matrix(P),3)
str(k)
P[,4]=k$centers
length(k$centers)
P[,4]=k$cluster
View(P)
ggplot(P)+geom_point(aes(x=V1,y=V2),colour=V4)
ggplot(P)+geom_point(aes(x=V1,y=V2))
names(P)
tab(P)
table(P$clust_ind,P$V4)
P =
do.call(
rbind,
rep(
list(
matrix(
rnorm(10, sd = 10),
ncol=2)),
20)) +
matrix(rnorm(200), ncol =2)
clust_ind=apply(P,1,function(x){
sub_dist=apply(outcome,1,function(y){
sqrt((y[1]-x[1])^2+(y[2]-x[2])^2)
})
which(sub_dist==min(sub_dist))
})
P=as.data.frame(P)
P[,3]=clust_ind
k=kmeans(as.matrix(P),3)
P[,4]=k$cluster
View(P)
table(P$V3,P$V4)
ggplot(P)+geom_point(aes(x=V1,y=V2),colour=V3)
ggplot(P)+geom_point(aes(x=V1,y=V2),colour=V4)
ggplot(P)+geom_point(aes(x=V1,y=V2),colour=P[,3])
ggplot(P)+geom_point(aes(x=V1,y=V2),colour=P[,4])
set.seed(3)
P =
do.call(
rbind,
rep(
list(
matrix(
rnorm(10, sd = 10),
ncol=2)),
20)) +
matrix(rnorm(200), ncol =2)
plot(P)
outcome=    kmeans.mr(
to.dfs(P),
num.clusters = 4,
num.iter = 5,
combine = FALSE,
in.memory.combine = FALSE)
clust_ind=apply(P,1,function(x){
sub_dist=apply(outcome,1,function(y){
sqrt((y[1]-x[1])^2+(y[2]-x[2])^2)
})
which(sub_dist==min(sub_dist))
})
P=as.data.frame(P)
P[,3]=clust_ind
k=kmeans(as.matrix(P),3)
P[,4]=k$cluster
library(ggplot2)
table(P$V3,P$V4)
ggplot(P)+geom_point(aes(x=V1,y=V2),colour=P[,3])
k=kmeans(as.matrix(P),4)
P[,4]=k$cluster
table(P$V3,P$V4)
ggplot(P)+geom_point(aes(x=V1,y=V2),colour=P[,3])
ggplot(P)+geom_point(aes(x=V1,y=V2),colour=P[,4])
set.seed(3)
P =
do.call(
rbind,
rep(
list(
matrix(
rnorm(10, sd = 10),
ncol=2)),
20)) +
matrix(rnorm(200), ncol =2)
plot(P)
outcome=    kmeans.mr(
to.dfs(P),
num.clusters = 4,
num.iter = 5,
combine = FALSE,
in.memory.combine = FALSE)
clust_ind=apply(P,1,function(x){
sub_dist=apply(outcome,1,function(y){
sqrt((y[1]-x[1])^2+(y[2]-x[2])^2)
})
which(sub_dist==min(sub_dist))
})
P=as.data.frame(P)
P[,3]=clust_ind
k=kmeans(as.matrix(P[,1:2]),4)
P[,4]=k$cluster
library(ggplot2)
table(P$V3,P$V4)
ggplot(P)+geom_point(aes(x=V1,y=V2),colour=P[,3])
ggplot(P)+geom_point(aes(x=V1,y=V2),colour=P[,4])
